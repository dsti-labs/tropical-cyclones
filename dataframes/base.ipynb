{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle of our first dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical => OneHotEncoder or one dimension with different values (1, 2, 3, 4, etc.)\n",
    "- SEASON oneHotEncoder (4)\n",
    "- BASIN (7)\n",
    "- NATURE (6)\n",
    "\n",
    "Numeric => everything between 0 and 1\n",
    "- LAT\n",
    "- LON\n",
    "- WIND \n",
    "- DIST2LAND\n",
    "- STORM_SPEED\n",
    "- STORM_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "\n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go \n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../data/ibtracs.csv\", \n",
    "    skiprows=[1],\n",
    "    header=0,\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working on a copy of the dataset to preserve an untouched source of the dfcon\n",
    "df_ibtracs = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SP', 'SI', 'WP', 'EP', nan, 'NI', 'SA'], dtype=object)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ibtracs['BASIN'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing NULL values in BASIN to North Atlantic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = gpd.read_file('../data_visualization/shapefiles/goas_v01.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_basin_geo = basins.loc[basins['name']=='North Atlantic Ocean', 'geometry'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_basin_geom = na_basin_geo.simplify(tolerance=1, preserve_topology=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibtracs.loc[df_ibtracs['BASIN'].isnull(), 'BASIN'] = 'NA' # Correcting the Null values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing NULL values in BASIN to correct NA subbasin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "subbasins = gpd.read_file('../data_visualization/shapefiles/World_Seas_IHO_v3.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "subbasins['NAME_SHORT'] = subbasins['NAME'].apply(lambda x: ''.join([i[0].upper() for i in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "subbasins = subbasins[['NAME_SHORT', 'geometry']] # Keeping only relevent informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "subbasins[\"geometry\"] = subbasins[\"geometry\"].simplify(tolerance=1, preserve_topology=False) # Simplifying the geometry to fasten the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "subbasins = subbasins.loc[subbasins['geometry'].intersects(na_basin_geom)].reset_index(drop=True) # Keeping only subbasins interesting with the NA basin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now complete the subbasin name information with the correct geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subbasin(point):\n",
    "    \"\"\"Fuction checking in wich subbasin a point is located, returning np.nan if none\"\"\"\n",
    "    subbasin_serie = subbasins.loc[subbasins['geometry'].contains(point), 'NAME_SHORT']\n",
    "    return np.nan if len(subbasin_serie)==0 else subbasin_serie.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning every LAT and LON of null subbasing points into Point objects\n",
    "point_serie = df_ibtracs.loc[df_ibtracs['SUBBASIN'].isnull(), ['LAT', 'LON']].apply(lambda x: Point(x['LON'], x['LAT']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the corresponding subbasin\n",
    "subbasin_serie_found = point_serie.apply(lambda p: find_subbasin(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completing the information in the original dataframe\n",
    "df_ibtracs.loc[df_ibtracs['SUBBASIN'].isnull(), \"SUBBASIN\"] = subbasin_serie_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibtracs.loc[df_ibtracs['SUBBASIN'].isnull(), \"SUBBASIN\"] = 'LAND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SP', 'SI', 'WP', 'EP', 'NA', 'NI', 'SA'], dtype=object)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ibtracs['BASIN'].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_ibtracs.columns:\n",
    "    df_ibtracs.loc[df_ibtracs[col]==\" \", col] = np.nan\n",
    "    try:\n",
    "        df_ibtracs[col] = pd.to_numeric(df_ibtracs[col])\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SP', 'SI', 'WP', 'EP', 'NI'], dtype=object)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ibtracs['BASIN'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibtracs = df_ibtracs.dropna(subset=[\"TD9636_STAGE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibtracs['ISO_TIME'] = pd.to_datetime(df_ibtracs['ISO_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date, latitude):\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "\n",
    "    if latitude >= 0:  # Northern Hemisphere\n",
    "        match month:\n",
    "            case 12 | 1 | 2: \n",
    "                return \"Winter\"\n",
    "            case 3 | 4 | 5: \n",
    "                return \"Spring\"\n",
    "            case 6 | 7 | 8:\n",
    "                return \"Summer\"\n",
    "            case 9 | 10 | 11: \n",
    "                return \"Fall\"\n",
    "    \n",
    "    else:  # Southern Hemisphere\n",
    "        match month:\n",
    "            case 12 | 1 | 2:\n",
    "                return \"Summer\"\n",
    "            case 3 | 4 | 5: \n",
    "                return  \"Fall\"\n",
    "            case 6 | 7 | 8: \n",
    "                return \"Winter\"\n",
    "            case 9 | 10 | 11: \n",
    "                return \"Spring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibtracs['SEASON'] = df_ibtracs.apply(lambda row: get_season(row['ISO_TIME'], row['LAT']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming USA_WIND as WIND\n",
    "df_ibtracs.rename(columns={\"USA_WIND\": \"WIND\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibtracs[[\"BOM_WIND\", \"WELLINGTON_WIND\", \"REUNION_WIND\", \"HKO_WIND\", \"TOKYO_WIND\"]] = df_ibtracs[[\"BOM_WIND\", \"WELLINGTON_WIND\", \"REUNION_WIND\", \"HKO_WIND\", \"TOKYO_WIND\"]] * 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_list = [            \n",
    "    \"TD9636_WIND\",                 \n",
    "    \"NEUMANN_WIND\",       \n",
    "    \"HKO_WIND\",           \n",
    "    \"TOKYO_WIND\",        \n",
    "    \"BOM_WIND\",          \n",
    "    \"WELLINGTON_WIND\",    \n",
    "    \"REUNION_WIND\",     \n",
    "    \"DS824_WIND\"        \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if WIND is NaN we loop over the other columns to fill it \n",
    "for column in wind_list:\n",
    "    df_ibtracs[\"WIND\"] = df_ibtracs[\"WIND\"].fillna(value=df_ibtracs[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where WIND is still null\n",
    "df_ibtracs = df_ibtracs.dropna(subset=['WIND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_ibtracs[[\n",
    "    \"SEASON\", \n",
    "    \"BASIN\", \n",
    "    \"NATURE\",\n",
    "    \"LAT\", \n",
    "    \"LON\", \n",
    "    \"WIND\", \n",
    "    \"DIST2LAND\",\n",
    "    \"STORM_SPEED\",\n",
    "    \"STORM_DIR\", \n",
    "    \"TD9636_STAGE\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45025 entries, 0 to 67409\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   SEASON        45025 non-null  object \n",
      " 1   BASIN         45025 non-null  object \n",
      " 2   NATURE        45025 non-null  object \n",
      " 3   LAT           45025 non-null  float64\n",
      " 4   LON           45025 non-null  float64\n",
      " 5   WIND          45025 non-null  float64\n",
      " 6   DIST2LAND     45025 non-null  float64\n",
      " 7   STORM_SPEED   45025 non-null  float64\n",
      " 8   STORM_DIR     45025 non-null  float64\n",
      " 9   TD9636_STAGE  45025 non-null  float64\n",
      "dtypes: float64(7), object(3)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEASON          0\n",
       "BASIN           0\n",
       "NATURE          0\n",
       "LAT             0\n",
       "LON             0\n",
       "WIND            0\n",
       "DIST2LAND       0\n",
       "STORM_SPEED     0\n",
       "STORM_DIR       0\n",
       "TD9636_STAGE    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.to_parquet(\"../data/base2.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SP', 'SI', 'WP', 'EP', 'NI'], dtype=object)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['BASIN'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
