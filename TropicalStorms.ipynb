{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of tropical storms intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages loading\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\_DSTI\\A24_Python_Machine_learning\\Project3_Tropical_storms\n"
     ]
    }
   ],
   "source": [
    "# Change current working directory\n",
    "# current directory:\n",
    "print(os.getcwd())\n",
    "# set new directory\n",
    "os.chdir('D:\\_DSTI\\A24_Python_Machine_learning\\Project3_Tropical_storms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading\n",
    "IBTrACS dataset from NOAA.\n",
    "International Best Track Archive for Climate Stewardship.\n",
    "https://www.ncei.noaa.gov/products/international-best-track-archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "df = pd.read_csv('.\\ibtracs.csv', header=0) \n",
    "df = df.iloc[1:] # remove 2nd line (with index 1).\n",
    "df = df.reset_index() #reset indexes ;\n",
    "# Add a column \"Index\" corresponding to the previous indexes. Can be removed.\n",
    "#storms.head()\n",
    "#print(storms)\n",
    "# storms: dataframe 297098 lines, 175 columns (174 + 1 for old indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns\n",
    "columns_headers=list(df.columns)\n",
    "print(columns_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM: \n",
    "Empty cells are in fact filled with whitespace and thus NA are not properly detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # it seems that there is no NA, which is false.\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x) # remove whitespace from every cells\n",
    "df.replace('', np.nan, inplace=True) # replace empty cells with NaN\n",
    "df_NA = df.isna().sum() # it seems to work. \n",
    "# We can remove all columns filled only with NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types:\n",
    "When loading the table, because of the 2nd row of units, Python do not detect datatypes and everything is considered as \"object\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data types : \n",
    "cols = df.filter(like=\"_WIND\").columns\n",
    "df[cols] = df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featuring engineering\n",
    "Columns to remove:\n",
    "    # index: column added by Python when updating indexes after removing the 2nd line.\n",
    "    # NAME: same as SID except that some storms don't have name. Redundant info\n",
    "    # NUMBER: number of the storm for the year. Restart at 1 each year. Not useful.\n",
    "    # REUNION_R64_NE\n",
    "    # REUNION_R64_SE\n",
    "    # REUNION_R64_SW\n",
    "    # REUNION_R64_NW\n",
    "    # REUNION_GUST\n",
    "    # TD9636_PRES\n",
    "    # TD9635_WIND\n",
    "    # TD9635_PRES\n",
    "    # TD9635_ROCI\n",
    "    # MLC_CLASS\n",
    "    # MLC_WIND\n",
    "    # MLC_PRES ----- All this previous columns are only empty cells\n",
    "    # ALL _LAT: all latitude are more or less the same as in column LAT. Redundant info.\n",
    "    # ALL _LON: all longitude are more or less the same as in column LON. Redundant info.\n",
    "    # BASIN \n",
    "    # SUBBASIN: These two will probably be correlated to LAT and LON. Redundant info.\n",
    "    # WMO_WIND: all data from agencies but differents units but no adjustement is made for differences in wind speed averaging. Data not comparable.\n",
    "    # TD9636_WIND: in data description: \"subjective, must be interpreted with caution\". Not reliable.\n",
    "\n",
    "\n",
    "Parameters common to all agencies: WIND and PRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are not useful/full of NA/not reliable:\n",
    "STORMS = df.drop(columns=['index', 'NAME', 'NUMBER',\n",
    "                                    'REUNION_R64_NE', 'REUNION_R64_SE', 'REUNION_R64_SW', 'REUNION_R64_NW', \n",
    "                                    'REUNION_GUST', 'TD9636_PRES', 'TD9635_WIND', 'TD9635_PRES', 'TD9635_ROCI', 'MLC_CLASS', 'MLC_WIND', \"MLC_PRES\",\n",
    "                                    'BASIN', 'SUBBASIN',\n",
    "                                    'WMO_WIND', 'TD9636_WIND',\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latitude and longitude columns\n",
    "We keep only LAT and LONG as it is the mean position (a combination of all the available positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latitude and longitude columns\n",
    "col_to_drop = STORMS.columns[STORMS.columns.str.contains('_LAT')] # all columns containing \"_LAT\"\n",
    "STORMS.drop(col_to_drop, axis=1, inplace=True) # remove all columns with _LAT\n",
    "col_to_drop = STORMS.columns[STORMS.columns.str.contains('_LON')] # all columns containing \"_LON\"\n",
    "STORMS.drop(col_to_drop, axis=1, inplace=True) # remove all columns with _LON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIND columns\n",
    "_WIND columsn from the different agencies have different units, and thus can not be gather together easily :\n",
    "    # WMO_WIND: all data from agencies but differents units (no adjustement is made for differences in wind speed averaging)\n",
    "    # I suggest to remove it. No reliable.\n",
    "    # USA_WIND, DS824_WIND, NEUMANN_WIND, max speed 1-min averaged\n",
    "    # CMA_WIND, max speed 2-min averaged\n",
    "    # NEWDELHI_WIND, max speed 3-min averaged\n",
    "    # TOKYO_WIND, HKO_WIND, KMA_WIND, REUNION_WIND, BOM_WIND, NADI_WIND, WELLINGTON_WIND max speed 10-min averaged\n",
    "    # TD9636_WIND: in data description \"estimates subjective, interpreted with caution\". Remove it. \n",
    "    # TD9635_WIND, MLC_WIND: only NA\n",
    "\n",
    "We can take USA_WIND as a base because it is where there is the less NAN and complete the missing data with other columns, especially DS824_WIND and NEUMANN_WIND as it is supposed to be the same unit (max speed 1-min averaged).\n",
    "Problem: data are not in the same unit \n",
    "To convert from max speed 10-min averaged to 1-min averaged : * 1.12\n",
    "IBTrACS technical details p5: \n",
    "    \"The U.S. agencies (NOAA and JTWC) report a 1 min averaging time for the sustained (i.e. relatively long-lasting) winds. In most of the rest of the world, a 10 min averaging time is used for \"sustained wind\". It is possible to convert from peak 10 min wind to peak 1 min wind (roughly 12% higher for the latter) as a general rule.\"\n",
    "It is not a very accurate method so I suggest not to use it too much.\n",
    "(See part 6.2 Wind speed reporting differences from Technical details doc)\n",
    "\n",
    "We create one column WIND to gather all wind data and one column AGENCY to track the origin of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two columns \"WIND\" and \"AGENCY\":\n",
    "STORMS[\"WIND\"] = np.nan\n",
    "STORMS[\"WIND_AGENCY\"] = np.nan\n",
    "\n",
    "sources_adj = [\n",
    "    (\"USA_WIND\", 1.0),  # No adjustment\n",
    "    (\"DS824_WIND\", 1.0),  # No adjustment\n",
    "    (\"NEUMANN_WIND\", 1.0),  # No adjustment\n",
    "    (\"TOKYO_WIND\", 1.12),  # Adjustment factor\n",
    "    (\"HKO_WIND\", 1.12),\n",
    "    (\"KMA_WIND\", 1.12),\n",
    "    (\"REUNION_WIND\", 1.12),\n",
    "    (\"BOM_WIND\", 1.12),\n",
    "    (\"NADI_WIND\", 1.12),\n",
    "    (\"WELLINGTON_WIND\", 1.12)\n",
    "]\n",
    "\n",
    "# Iterate through the sources\n",
    "for source, factor in sources_adj:\n",
    "    mask = pd.isna(STORMS[\"WIND\"]) & ~pd.isna(STORMS[source])  # Where WIND is NaN but source is not NaN\n",
    "    STORMS.loc[mask, \"WIND\"] = STORMS.loc[mask, source] * factor\n",
    "    STORMS.loc[mask, \"WIND_AGENCY\"] = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can remove all other _WIND columns\n",
    "col_to_drop = STORMS.columns[STORMS.columns.str.contains('_WIND')] # all columns containing \"_WIND\"\n",
    "STORMS.drop(col_to_drop, axis=1, inplace=True) # remove all columns with _WIND\n",
    "STORMS.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Detailed explaination for removing columns\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of storms\n",
    "IDS = df.loc[:, [\"SID\", \"NAME\", \"NUMBER\"]] \n",
    "# SID and Name are the same except that some storms don't have names so let's keep SID and remove Name\n",
    "# NUMBER is the number of the storm for the year. Restart at 1 each year. Remove it. \n",
    "IDS.describe() # There are 4767 different storms\n",
    "IDS.isna().sum() # No NA in SID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATITUDE\n",
    "LAT = df.filter(like=\"LAT\", axis=1) # All columns containing \"LAT\" in their headers\n",
    "# All latitudes are the same most of the time (+- 0.1, sometimes more but mostly the same).\n",
    "# All these columns contains the same information. \n",
    "LAT = LAT.loc[:, ['LAT']]\n",
    "LAT.isna().sum() # No na in column LAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LONGITUDE\n",
    "LON = df.filter(like=\"LON\", axis=1)\n",
    "LON.isna().sum()\n",
    "# same as with LAT, all longitudes are the same most of the time (+- 0.1, sometimes more but mostly the same).\n",
    "# All these columns contains the same information. \n",
    "LON = LON.loc[:, ['LON']]\n",
    "LON.isna().sum() # No na in column LAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data by location and parameters\n",
    "STORMS_NA = STORMS.iloc[:, 13:122] # Extract columns with parameters per location\n",
    "LOC_PARAM = [col.split(\"_\", 1) \n",
    "         for col in STORMS_NA.columns] # split headers based on the first _ to split LOC and PARAM\n",
    "LOC = [p[0] for p in LOC_PARAM] # extract Location from column names\n",
    "LOC_unique = sorted(set(LOC)) # give a list of unique location\n",
    "PARAM = [p[1] for p in LOC_PARAM] # extract parameters from column names\n",
    "PARAM_unique = sorted(set(PARAM)) # give a list of unique parameters\n",
    "\n",
    "NA = pd.DataFrame(0, index=LOC_unique, columns=PARAM_unique) # Prepare the dataframe to be filled with sum of na\n",
    "\n",
    "for col in STORMS_NA.columns:\n",
    "    LOC, PARAM = col.split('_', 1)  # Split only at the first underscore\n",
    "    NA.loc[LOC, PARAM] += STORMS_NA[col].isna().sum() # sum of NA values\n",
    "# Not very helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_6364\\535304703.py:4: DtypeWarning: Columns (1,2,8,9,14,19,20,172,173) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('.\\ibtracs.csv', header=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'SID', 'SEASON', 'NUMBER', 'BASIN', 'SUBBASIN', 'NAME', 'ISO_TIME', 'NATURE', 'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'WMO_AGENCY', 'TRACK_TYPE', 'DIST2LAND', 'LANDFALL', 'IFLAG', 'USA_AGENCY', 'USA_ATCF_ID', 'USA_LAT', 'USA_LON', 'USA_RECORD', 'USA_STATUS', 'USA_WIND', 'USA_PRES', 'USA_SSHS', 'USA_R34_NE', 'USA_R34_SE', 'USA_R34_SW', 'USA_R34_NW', 'USA_R50_NE', 'USA_R50_SE', 'USA_R50_SW', 'USA_R50_NW', 'USA_R64_NE', 'USA_R64_SE', 'USA_R64_SW', 'USA_R64_NW', 'USA_POCI', 'USA_ROCI', 'USA_RMW', 'USA_EYE', 'TOKYO_LAT', 'TOKYO_LON', 'TOKYO_GRADE', 'TOKYO_WIND', 'TOKYO_PRES', 'TOKYO_R50_DIR', 'TOKYO_R50_LONG', 'TOKYO_R50_SHORT', 'TOKYO_R30_DIR', 'TOKYO_R30_LONG', 'TOKYO_R30_SHORT', 'TOKYO_LAND', 'CMA_LAT', 'CMA_LON', 'CMA_CAT', 'CMA_WIND', 'CMA_PRES', 'HKO_LAT', 'HKO_LON', 'HKO_CAT', 'HKO_WIND', 'HKO_PRES', 'KMA_LAT', 'KMA_LON', 'KMA_CAT', 'KMA_WIND', 'KMA_PRES', 'KMA_R50_DIR', 'KMA_R50_LONG', 'KMA_R50_SHORT', 'KMA_R30_DIR', 'KMA_R30_LONG', 'KMA_R30_SHORT', 'NEWDELHI_LAT', 'NEWDELHI_LON', 'NEWDELHI_GRADE', 'NEWDELHI_WIND', 'NEWDELHI_PRES', 'NEWDELHI_CI', 'NEWDELHI_DP', 'NEWDELHI_POCI', 'REUNION_LAT', 'REUNION_LON', 'REUNION_TYPE', 'REUNION_WIND', 'REUNION_PRES', 'REUNION_TNUM', 'REUNION_CI', 'REUNION_RMW', 'REUNION_R34_NE', 'REUNION_R34_SE', 'REUNION_R34_SW', 'REUNION_R34_NW', 'REUNION_R50_NE', 'REUNION_R50_SE', 'REUNION_R50_SW', 'REUNION_R50_NW', 'REUNION_R64_NE', 'REUNION_R64_SE', 'REUNION_R64_SW', 'REUNION_R64_NW', 'BOM_LAT', 'BOM_LON', 'BOM_TYPE', 'BOM_WIND', 'BOM_PRES', 'BOM_TNUM', 'BOM_CI', 'BOM_RMW', 'BOM_R34_NE', 'BOM_R34_SE', 'BOM_R34_SW', 'BOM_R34_NW', 'BOM_R50_NE', 'BOM_R50_SE', 'BOM_R50_SW', 'BOM_R50_NW', 'BOM_R64_NE', 'BOM_R64_SE', 'BOM_R64_SW', 'BOM_R64_NW', 'BOM_ROCI', 'BOM_POCI', 'BOM_EYE', 'BOM_POS_METHOD', 'BOM_PRES_METHOD', 'NADI_LAT', 'NADI_LON', 'NADI_CAT', 'NADI_WIND', 'NADI_PRES', 'WELLINGTON_LAT', 'WELLINGTON_LON', 'WELLINGTON_WIND', 'WELLINGTON_PRES', 'DS824_LAT', 'DS824_LON', 'DS824_STAGE', 'DS824_WIND', 'DS824_PRES', 'TD9636_LAT', 'TD9636_LON', 'TD9636_STAGE', 'TD9636_WIND', 'TD9636_PRES', 'TD9635_LAT', 'TD9635_LON', 'TD9635_WIND', 'TD9635_PRES', 'TD9635_ROCI', 'NEUMANN_LAT', 'NEUMANN_LON', 'NEUMANN_CLASS', 'NEUMANN_WIND', 'NEUMANN_PRES', 'MLC_LAT', 'MLC_LON', 'MLC_CLASS', 'MLC_WIND', 'MLC_PRES', 'USA_GUST', 'BOM_GUST', 'BOM_GUST_PER', 'REUNION_GUST', 'REUNION_GUST_PER', 'USA_SEAHGT', 'USA_SEARAD_NE', 'USA_SEARAD_SE', 'USA_SEARAD_SW', 'USA_SEARAD_NW', 'STORM_SPEED', 'STORM_DIR']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_6364\\535304703.py:18: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x) # remove whitespace from every cells\n",
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_6364\\535304703.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('', np.nan, inplace=True) # replace empty cells with NaN\n",
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_6364\\535304703.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  STORMS[\"WIND\"] = np.nan\n",
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_6364\\535304703.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  STORMS[\"WIND_AGENCY\"] = np.nan\n",
      "C:\\Users\\Claire\\AppData\\Local\\Temp\\ipykernel_6364\\535304703.py:89: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'USA_WIND' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  STORMS.loc[mask, \"WIND_AGENCY\"] = source\n"
     ]
    }
   ],
   "source": [
    "# WIND\n",
    "    # WMO_WIND: all data from agencies but differents units (no adjustement is made for differences in wind speed averaging)\n",
    "    # I suggest to remove it. No reliable.\n",
    "    # USA_WIND, DS824_WIND, NEUMANN_WIND, max speed 1-min averaged\n",
    "    # CMA_WIND, max speed 2-min averaged\n",
    "    # NEWDELHI_WIND, max speed 3-min averaged\n",
    "    # TOKYO_WIND, HKO_WIND, KMA_WIND, REUNION_WIND, BOM_WIND, NADI_WIND, WELLINGTON_WIND max speed 10-min averaged\n",
    "    # TD9636_WIND: in data description \"estimates subjective, interpreted with caution\". Remove it. \n",
    "    # TD9635_WIND, MLC_WIND: only NA\n",
    "WIND = STORMS.filter(like=\"WIND\", axis =1)\n",
    "WIND.isna().sum()\n",
    "WIND.dtypes\n",
    "# convert wind to numeric\n",
    "cols = WIND.columns\n",
    "WIND[cols] = WIND[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    # We can take USA_WIND as a base because it is where there is the less NAN and complete the missing data with other columns, \n",
    "    # especially DS824_WIND and NEUMANN_WIND as it is supposed to be the same unit (max speed 1-min averaged).\n",
    "\"\"\"\n",
    "WIND.insert(0, \"WIND\", \"\")\n",
    "WIND.insert(1, \"AGENCY\", \"\")\n",
    "\n",
    "for i in WIND.index:\n",
    "    if not pd.isna(WIND.at[i, \"USA_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = WIND.at[i, \"USA_WIND\"]\n",
    "        WIND.at[i, \"AGENCY\"] = \"USA_WIND\" \n",
    "    elif pd.isna(WIND.at[i, \"USA_WIND\"]) and not pd.isna(WIND.at[i, \"DS824_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = WIND.at[i, \"DS824_WIND\"]\n",
    "        WIND.at[i, \"AGENCY\"] = \"DS824_WIND\"\n",
    "    elif pd.isna(WIND.at[i, \"USA_WIND\"]) and pd.isna(WIND.at[i, \"DS824_WIND\"]) and not pd.isna(WIND.at[i, \"NEUMANN_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = WIND.at[i, \"NEUMANN_WIND\"]\n",
    "        WIND.at[i, \"AGENCY\"] = \"NEUMANN_WIND\"\n",
    "    else:\n",
    "        WIND.at[i, \"WIND\"] = np.nan\n",
    "        WIND.at[i, \"AGENCY\"] = np.nan\n",
    "WIND.isna().sum()\n",
    "\"\"\"\n",
    "# We don't fill much more data\n",
    "# WIND.dropna(axis = 0, how='all', inplace = True) # inplace = True is to update the table\n",
    "# WIND.isna().sum()\n",
    "# When removing rows with only NA (thus with no data for wind) we still have 30080 rows with NA in WIND and with data somewherelse we could use.\n",
    "# Problem: data are not in the same unit \n",
    "# To convert from max speed 10-min averaged to 1-min averaged : * 1.12\n",
    "# IBTrACS technical details p5: \n",
    "    # \"The U.S. agencies (NOAA and JTWC) report a 1 min averaging time for the sustained (i.e. relatively long-lasting) winds. \n",
    "    # In most of the rest of the world, a 10 min averaging time is used for \"sustained wind\". \n",
    "    # It is possible to convert from peak 10 min wind to peak 1 min wind (roughly 12% higher for the latter) as a general rule.\"\n",
    "# It is not a very accurate method so I suggest not to use it too much.\n",
    "# (See part 6.2 Wind speed reporting differences from Technical details doc)\n",
    "\"\"\"\n",
    "for i in WIND.index:\n",
    "    if pd.isna(WIND.at[i, \"USA_WIND\"]) and not pd.isna(WIND.at[i, \"TOKYO_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = (WIND.at[i, \"TOKYO_WIND\"] * 1.12)\n",
    "        WIND.at[i, \"AGENCY\"] = \"TOKYO_WIND\"\n",
    "    elif pd.isna(WIND.at[i, \"USA_WIND\"]) and not pd.isna(WIND.at[i, \"HKO_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = (WIND.at[i, \"HKO_WIND\"] * 1.12)\n",
    "        WIND.at[i, \"AGENCY\"] = \"HKO_WIND\"\n",
    "    elif pd.isna(WIND.at[i, \"USA_WIND\"]) and not pd.isna(WIND.at[i, \"KMA_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = (WIND.at[i, \"KMA_WIND\"] * 1.12)\n",
    "        WIND.at[i, \"AGENCY\"] = \"KMA_WIND\"\n",
    "    elif pd.isna(WIND.at[i, \"USA_WIND\"]) and not pd.isna(WIND.at[i, \"REUNION_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = (WIND.at[i, \"REUNION_WIND\"] * 1.12)\n",
    "        WIND.at[i, \"AGENCY\"] = \"REUNION_WIND\"\n",
    "    elif pd.isna(WIND.at[i, \"USA_WIND\"]) and not pd.isna(WIND.at[i, \"BOM_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = (WIND.at[i, \"BOM_WIND\"] * 1.12)\n",
    "        WIND.at[i, \"AGENCY\"] = \"BOM_WIND\"\n",
    "    elif pd.isna(WIND.at[i, \"USA_WIND\"]) and not pd.isna(WIND.at[i, \"NADI_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = (WIND.at[i, \"NADI_WIND\"] * 1.12)\n",
    "        WIND.at[i, \"AGENCY\"] = \"NADI_WIND\"\n",
    "    elif pd.isna(WIND.at[i, \"USA_WIND\"]) and not pd.isna(WIND.at[i, \"WELLINGTON_WIND\"]):\n",
    "        WIND.at[i, \"WIND\"] = (WIND.at[i, \"WELLINGTON_WIND\"] * 1.12)\n",
    "        WIND.at[i, \"AGENCY\"] = \"WELLINGTON_WIND\" \n",
    "WIND.isna().sum() \n",
    "\"\"\"\n",
    "\n",
    "# Or in a cleaner way:\n",
    "sources_adj = [\n",
    "    (\"USA_WIND\", 1.0),  # No adjustment\n",
    "    (\"DS824_WIND\", 1.0),  # No adjustment\n",
    "    (\"NEUMANN_WIND\", 1.0),  # No adjustment\n",
    "    (\"TOKYO_WIND\", 1.12),  # Adjustment factor\n",
    "    (\"HKO_WIND\", 1.12),\n",
    "    (\"KMA_WIND\", 1.12),\n",
    "    (\"REUNION_WIND\", 1.12),\n",
    "    (\"BOM_WIND\", 1.12),\n",
    "    (\"NADI_WIND\", 1.12),\n",
    "    (\"WELLINGTON_WIND\", 1.12)\n",
    "]\n",
    "\n",
    "# Create the two columns \"WIND\" and \"AGENCY:\n",
    "WIND[\"WIND\"] = np.nan\n",
    "WIND[\"WIND_AGENCY\"] = np.nan\n",
    "\n",
    "# Iterate through the sources\n",
    "for source, factor in sources_adj:\n",
    "    mask = pd.isna(WIND[\"WIND\"]) & ~pd.isna(WIND[source])  # Where WIND is NaN but source is not NaN\n",
    "    WIND.loc[mask, \"WIND\"] = WIND.loc[mask, source] * factor\n",
    "    WIND.loc[mask, \"WIND_AGENCY\"] = source\n",
    "WIND.dropna(axis = 0, how='all', inplace = True)\n",
    "WIND.isna().sum()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0910_WarmUp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
